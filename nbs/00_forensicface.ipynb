{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forensicface--A tool for forensic face examination\n",
    "\n",
    "> An integrated tool to compare faces using state-of-the-art face recognition models and compute Likelihood Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *\n",
    "import onnxruntime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from imutils import build_montages\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ForensicFace:\n",
    "    \"\"\"\n",
    "    Class for processing facial images to extract useful features for forensic analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"sepaelv2\",\n",
    "        det_size: int = 320,\n",
    "        use_gpu: bool = True,\n",
    "        gpu: int = 0,  # which GPU to use\n",
    "        extended=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A face comparison tool for forensic analysis and comparison of facial images.\n",
    "\n",
    "        Args:\n",
    "        - model (str): The name of the face recognition model to use (default: \"sepaelv2\").\n",
    "        - det_size (int): The size of the input images for face detection (default: 320).\n",
    "        - use_gpu (bool): Whether to use a GPU for inference (default: True).\n",
    "        - gpu (int): The ID of the GPU to use (default: 0).\n",
    "        - magface (bool): Whether to use MagFace for face recognition (default: False).\n",
    "        - extended (bool): Whether to use extended modules (detection, landmark_3d_68, genderage) (default: True).\n",
    "        \"\"\"\n",
    "        self.extended = extended\n",
    "        if self.extended == True:\n",
    "            allowed_modules = [\"detection\", \"landmark_3d_68\", \"genderage\"]\n",
    "            self.ort_fiqa = onnxruntime.InferenceSession(\n",
    "                osp.join(\n",
    "                    osp.expanduser(\"~/.insightface/models\"),\n",
    "                    model,\n",
    "                    \"cr_fiqa\",\n",
    "                    \"cr_fiqa_l.onnx\",\n",
    "                ),\n",
    "                providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "                if use_gpu\n",
    "                else [\"CPUExecutionProvider\"],\n",
    "            )\n",
    "        else:\n",
    "            allowed_modules = [\"detection\"]\n",
    "\n",
    "        self.det_size = (det_size, det_size)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.detectmodel = FaceAnalysis(\n",
    "            name=model,\n",
    "            allowed_modules=allowed_modules,\n",
    "            providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "        self.detectmodel.prepare(ctx_id=gpu if use_gpu else -1, det_size=self.det_size)\n",
    "\n",
    "        onnx_rec_model = glob(\n",
    "            osp.join(\n",
    "                osp.expanduser(\"~/.insightface/models\"),\n",
    "                model,\n",
    "                \"adaface\",\n",
    "                \"adaface_*.onnx\",\n",
    "            )\n",
    "        )\n",
    "        assert len(onnx_rec_model) == 1\n",
    "        self.ort_ada = onnxruntime.InferenceSession(\n",
    "            onnx_rec_model[0],\n",
    "            providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "\n",
    "    def _to_input_ada(self, aligned_bgr_img):\n",
    "        \"\"\"\n",
    "        Preprocesses the input face for the face recognition model.\n",
    "\n",
    "        Args:\n",
    "            face: Face image as a numpy array in BGR order.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed face image as a numpy array.\n",
    "        \"\"\"\n",
    "        _aligned_bgr_img = aligned_bgr_img.astype(np.float32)\n",
    "        _aligned_bgr_img = ((_aligned_bgr_img / 255.0) - 0.5) / 0.5\n",
    "        return _aligned_bgr_img.transpose(2, 0, 1).reshape(1, 3, 112, 112)\n",
    "\n",
    "    def get_most_central_face(self, img, faces):\n",
    "        \"\"\"\n",
    "        Get the keypoints of the most central face in an image.\n",
    "\n",
    "        Args:\n",
    "            img: Input image as a numpy array.\n",
    "            faces: An insightface object with keypoints and bounding_box.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing the index of the most central face and its keypoints.\n",
    "        \"\"\"\n",
    "        assert faces is not None\n",
    "        img_center = np.array([img.shape[0] // 2, img.shape[1] // 2])\n",
    "        dist = []\n",
    "\n",
    "        # Compute centers of faces and distances from certer of image\n",
    "        for idx, face in enumerate(faces):\n",
    "            box = face.bbox.astype(\"int\").flatten()\n",
    "            face_center = np.array([(box[0] + box[2]) // 2, (box[1] + box[3]) // 2])\n",
    "            dist.append(np.linalg.norm(img_center - face_center))\n",
    "\n",
    "        # Get index of the face closest to the center of image\n",
    "        idx = dist.index(min(dist))\n",
    "        return idx, faces[idx].kps\n",
    "\n",
    "    def get_larger_face(self, img, faces):\n",
    "        \"\"\"\n",
    "        Get the keypoints of the larger face in an image.\n",
    "\n",
    "        Args:\n",
    "            img: Input image as a numpy array.\n",
    "            faces: An insightface object with keypoints and bounding_box.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing the index of the larger face and its keypoints.\n",
    "        \"\"\"\n",
    "        assert faces is not None\n",
    "        areas = []\n",
    "\n",
    "        # Compute centers of faces and distances from certer of image\n",
    "        for idx, face in enumerate(faces):\n",
    "            box = face.bbox.astype(\"int\").flatten()\n",
    "            areas.append(abs((box[2] - box[0]) * (box[3] - box[1])))\n",
    "\n",
    "        # Get index of the face closest to the center of image\n",
    "        idx = areas.index(max(areas))\n",
    "        return idx, faces[idx].kps\n",
    "\n",
    "    def process_image_single_face(self, imgpath: str):  # Path to image to be processed\n",
    "        \"\"\"\n",
    "        Process a an image considering it has a single face and extract useful features for forensic analysis.\n",
    "\n",
    "        Args:\n",
    "            imgpath: Path to the input image.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the following keys:\n",
    "                - 'keypoints': A 2D numpy array of shape (5, 2) containing the facial keypoints\n",
    "                        for each face in the image. The keypoints are ordered as follows:\n",
    "                       left eye, right eye, nose tip, left mouth corner, and right mouth corner.\n",
    "\n",
    "                - 'ipd': A float representing the inter-pupillary distance for each face in the image.\n",
    "\n",
    "                - 'embedding': A 1D numpy array of shape (512,) containing the facial embedding\n",
    "                       for each face in the image.\n",
    "\n",
    "                - 'norm': A float representing the L2 norm of the embedding for each face in the image.\n",
    "\n",
    "                - 'bbox': A 1D numpy array of shape (4,) containing the bounding box coordinates for each face\n",
    "                  in the image. The coordinates are ordered as follows: (xmin, ymin, xmax, ymax).\n",
    "\n",
    "                - 'aligned_face': A 3D numpy array of shape (H, W, C) in RGB order containing the aligned face image for\n",
    "                          each face in the image. The image has been cropped and aligned based on the\n",
    "                          facial keypoints.\n",
    "\n",
    "                If the 'extended' attribute is set to True, the dictionary will also contain the following keys:\n",
    "                - 'gender': A string representing the gender for each face in the image.\n",
    "                               Possible values are 'M' for male and 'F' for female.\n",
    "\n",
    "                - 'age': An integer representing the estimated age for each face in the image.\n",
    "\n",
    "                - 'pitch': A float representing the pitch angle for each face in the image.\n",
    "\n",
    "                - 'yaw': A float representing the yaw angle for each face in the image.\n",
    "\n",
    "                - 'roll': A float representing the roll angle for each face in the image.\n",
    "\n",
    "                If the 'magface' attribute is set to True, the dictionary will also contain the following keys:\n",
    "                - 'magface_embedding': A 1D numpy array of shape (512,) containing the magface\n",
    "                                          embedding for each face in the image.\n",
    "\n",
    "                - 'magface_norm': A float representing the L2 norm of the magface embedding for\n",
    "                                     each face in the image.\n",
    "        \"\"\"\n",
    "        if type(imgpath) == str:  # image path passed as argument\n",
    "            bgr_img = cv2.imread(imgpath)\n",
    "        else:  # image array passed as argument\n",
    "            bgr_img = imgpath.copy()\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return {}\n",
    "\n",
    "        idx, kps = self.get_larger_face(bgr_img, faces)\n",
    "\n",
    "        bbox = faces[idx].bbox.astype(\"int\")\n",
    "        bgr_aligned_face = face_align.norm_crop(bgr_img, kps)\n",
    "        ipd = np.linalg.norm(kps[0] - kps[1])\n",
    "\n",
    "        ada_inputs = {\n",
    "            self.ort_ada.get_inputs()[0].name: self._to_input_ada(bgr_aligned_face)\n",
    "        }\n",
    "        normalized_embedding, norm = self.ort_ada.run(None, ada_inputs)\n",
    "\n",
    "        ret = {\n",
    "            \"keypoints\": kps,\n",
    "            \"ipd\": ipd,\n",
    "            \"embedding\": normalized_embedding.flatten() * norm.flatten()[0],\n",
    "            \"norm\": norm.flatten()[0],\n",
    "            \"bbox\": bbox,\n",
    "            \"aligned_face\": cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB),\n",
    "        }\n",
    "\n",
    "        if self.extended:\n",
    "            gender = \"M\" if faces[idx].gender == 1 else \"F\"\n",
    "            age = faces[idx].age\n",
    "            pitch, yaw, roll = faces[idx].pose\n",
    "            _, fiqa_score = self.ort_fiqa.run(None, ada_inputs)\n",
    "            ret = {\n",
    "                **ret,\n",
    "                **{\n",
    "                    \"gender\": gender,\n",
    "                    \"age\": age,\n",
    "                    \"pitch\": pitch,\n",
    "                    \"yaw\": yaw,\n",
    "                    \"roll\": roll,\n",
    "                    \"fiqa_score\": fiqa_score[0][0],\n",
    "                },\n",
    "            }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def process_image(self, imgpath):\n",
    "        return self.process_image_single_face(imgpath)\n",
    "\n",
    "    def process_image_multiple_faces(\n",
    "        self,\n",
    "        imgpath: str,  # Path to image to be processed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process an image with one or multiple faces and returns a list of dictionaries\n",
    "        with the following keys:\n",
    "\n",
    "            - 'keypoints': A 2D numpy array of shape (5, 2) containing the facial keypoints\n",
    "                            for each face in the image. The keypoints are ordered as follows:\n",
    "                            left eye, right eye, nose tip, left mouth corner, and right mouth corner.\n",
    "\n",
    "            - 'ipd': A float representing the inter-pupillary distance for each face in the image.\n",
    "\n",
    "            - 'embedding': A 1D numpy array of shape (512,) containing the facial embedding\n",
    "                            for each face in the image.\n",
    "\n",
    "            - 'norm': A float representing the L2 norm of the embedding for each face in the image.\n",
    "\n",
    "            - 'bbox': A 1D numpy array of shape (4,) containing the bounding box coordinates for each face\n",
    "                        in the image. The coordinates are ordered as follows: (xmin, ymin, xmax, ymax).\n",
    "\n",
    "            - 'aligned_face': A numpy array of shape (112, 112, 3) in RGB order containing the aligned face image for\n",
    "                                each face in the image. The image has been cropped and aligned based on the\n",
    "                                facial keypoints.\n",
    "\n",
    "         If the 'extended' attribute is set to True, the dictionaries will also contain the following keys:\n",
    "            - 'gender': A string representing the sex for each face in the image.\n",
    "                        Possible values are 'M' for male and 'F' for female.\n",
    "\n",
    "            - 'age': An integer representing the estimated age for each face in the image.\n",
    "\n",
    "            - 'pitch': A float representing the pitch angle for each face in the image.\n",
    "\n",
    "            - 'yaw': A float representing the yaw angle for each face in the image.\n",
    "\n",
    "            - 'roll: A float representing the roll angle for each face in the image.\n",
    "\n",
    "         If the 'magface' attribute is set to True, the dictionary will also contain the following keys:\n",
    "            - 'magface_embedding': A 1D numpy array of shape (512,) containing the magface\n",
    "                                    embedding for each face in the image.\n",
    "\n",
    "            - 'magface_norm': A float representing the L2 norm of the magface embedding for\n",
    "                                each face in the image.\n",
    "\n",
    "        Args:\n",
    "            - imgpath (str): The file path to the image to be processed.\n",
    "\n",
    "        Returns:\n",
    "            - A list of dictionaries, with each dictionary representing a face in the image.\n",
    "        \"\"\"\n",
    "        if type(imgpath) == str:  # image path passed as argument\n",
    "            bgr_img = cv2.imread(imgpath)\n",
    "        else:  # image array passed as argument\n",
    "            bgr_img = imgpath.copy()\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return []\n",
    "        ret = []\n",
    "        for face in faces:\n",
    "            kps = face.kps\n",
    "            bbox = face.bbox.astype(\"int\")\n",
    "            bgr_aligned_face = face_align.norm_crop(bgr_img, kps)\n",
    "            ipd = np.linalg.norm(kps[0] - kps[1])\n",
    "            ada_inputs = {\n",
    "                self.ort_ada.get_inputs()[0].name: self._to_input_ada(bgr_aligned_face)\n",
    "            }\n",
    "            normalized_embedding, norm = self.ort_ada.run(None, ada_inputs)\n",
    "            face_ret = {\n",
    "                \"keypoints\": kps,\n",
    "                \"ipd\": ipd,\n",
    "                \"embedding\": normalized_embedding.flatten() * norm.flatten()[0],\n",
    "                \"norm\": norm.flatten()[0],\n",
    "                \"bbox\": bbox,\n",
    "                \"aligned_face\": cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB),\n",
    "            }\n",
    "\n",
    "            if self.extended:\n",
    "                gender = \"M\" if face.gender == 1 else \"F\"\n",
    "                age = face.age\n",
    "                pitch, yaw, roll = face.pose\n",
    "                _, fiqa_score = self.ort_fiqa.run(None, ada_inputs)\n",
    "                face_ret = {\n",
    "                    **face_ret,\n",
    "                    **{\n",
    "                        \"gender\": gender,\n",
    "                        \"age\": age,\n",
    "                        \"pitch\": pitch,\n",
    "                        \"yaw\": yaw,\n",
    "                        \"roll\": roll,\n",
    "                        \"fiqa_score\": fiqa_score[0][0],\n",
    "                    },\n",
    "                }\n",
    "\n",
    "            ret.append(face_ret)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def build_mosaic(self, img_path_list, mosaic_shape, border=0.03, save_to=None):\n",
    "        \"\"\"\n",
    "        Build a rectangular mosaic of the aligned faces.\n",
    "        Based on the imutils build_montages function.\n",
    "\n",
    "        Parameters:\n",
    "            img_path_list: list of paths to image files\n",
    "            mosaic_shape: tuple of integers, (n_cols, n_rows)\n",
    "            border: float, percent of image to use as white border\n",
    "\n",
    "        Returns:\n",
    "            cv2 BGR image with mosaic\n",
    "        \"\"\"\n",
    "        assert mosaic_shape is not None\n",
    "        top = int(border * 112)  # shape[0] = rows\n",
    "        bottom = top\n",
    "        left = int(border * 112)  # shape[1] = cols\n",
    "        right = left\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in img_path_list:\n",
    "            ret = self.process_image_single_face(img_path)\n",
    "            if len(ret) > 0:\n",
    "                img = cv2.cvtColor(ret[\"aligned_face\"], cv2.COLOR_RGB2BGR)\n",
    "                img = cv2.copyMakeBorder(\n",
    "                    img,\n",
    "                    top=top,\n",
    "                    bottom=bottom,\n",
    "                    left=left,\n",
    "                    right=right,\n",
    "                    borderType=cv2.BORDER_CONSTANT,\n",
    "                    value=(255, 255, 255),\n",
    "                )\n",
    "                imgs.append(img)\n",
    "        mosaic = build_montages(\n",
    "            imgs,\n",
    "            image_shape=(int(112 * (1 + 2 * border)), int(112 * (1 + 2 * border))),\n",
    "            montage_shape=mosaic_shape,\n",
    "        )[0]\n",
    "        if save_to is not None:\n",
    "            cv2.imwrite(save_to, mosaic)\n",
    "        return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
=======
      "Deprecation warning: support for magface models will be discontinued soon.\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "ff = ForensicFace(use_gpu=True, magface=False, extended=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramys\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keypoints': array([[467.36655, 681.6384 ],\n",
       "        [772.4217 , 677.0312 ],\n",
       "        [625.3773 , 836.4494 ],\n",
       "        [504.2983 , 992.18964],\n",
       "        [751.0406 , 985.683  ]], dtype=float32),\n",
       " 'ipd': 305.08994,\n",
       " 'embedding': array([ 0.31487796,  0.38824913, -0.7322802 , -1.4290268 ,  0.1157581 ,\n",
       "         0.52496874,  0.2104893 , -0.06046099, -0.6671673 , -1.1109946 ,\n",
       "        -0.27294186,  0.4760788 ,  0.05447045,  0.7953767 , -1.3528583 ,\n",
       "         0.5849594 ,  0.20620403, -0.14711168,  1.8467338 ,  0.699105  ,\n",
       "        -1.2522143 ,  2.1628525 ,  0.5511953 ,  0.8415365 , -0.39673612,\n",
       "         0.21230443, -0.02343156,  0.8276689 ,  0.08805086, -0.0412251 ,\n",
       "         0.47658077, -0.7329929 ,  0.16458814,  0.17394914, -0.06392059,\n",
       "        -0.02962815, -0.04553621,  0.89879787, -1.0760044 ,  0.04898747,\n",
       "        -1.358807  , -0.95639193, -0.1286546 , -0.22066405,  1.1417154 ,\n",
       "         1.1835935 ,  0.29173243, -0.29254076,  0.06820393, -0.09867842,\n",
       "         0.07090543,  1.0653521 ,  0.6491635 ,  0.6983762 ,  0.01935001,\n",
       "         0.58302176,  0.45900765, -0.32227117,  0.07045601, -0.1428774 ,\n",
       "        -0.37476262, -0.29461685,  0.22729869, -0.90549785,  0.05654888,\n",
       "        -0.52063054,  0.01600378, -0.09696029, -0.45387018, -0.32460493,\n",
       "         0.8288934 , -0.13826464,  0.95678234,  0.41111138,  0.46442273,\n",
       "         0.21031353, -0.4478161 ,  0.44371632,  0.19150586, -0.96256125,\n",
       "         0.8830052 ,  0.64384496, -0.3620545 , -0.33799896,  0.27204135,\n",
       "         0.92645645, -0.5289957 ,  0.6848778 , -0.71221966, -0.07938752,\n",
       "        -1.6151408 ,  0.06805508,  0.07911402,  0.32610834,  0.99082524,\n",
       "        -0.21381228,  0.09903055, -0.15241012,  0.30216366, -0.82938385,\n",
       "        -0.9411897 , -0.1634742 , -0.2643879 ,  0.43955463,  0.8186995 ,\n",
       "        -0.9650563 , -0.41002002,  0.38406992, -0.6898056 ,  0.41387293,\n",
       "         0.17989871, -1.0634441 ,  0.3327508 , -0.2250269 ,  1.4118869 ,\n",
       "        -1.2528278 ,  0.65876883, -0.13873002, -1.0977908 , -0.88466537,\n",
       "         0.18080805,  0.595875  ,  0.2885907 , -0.13044117,  0.25219667,\n",
       "        -0.4486985 ,  0.3266388 ,  0.36621243, -0.24285047, -0.00535857,\n",
       "        -1.0802994 ,  0.68160343,  0.54547817, -0.73706275, -0.1667702 ,\n",
       "         0.6299172 ,  1.8768425 ,  0.6780384 , -0.01420384, -0.88057184,\n",
       "        -0.27060413,  1.2802252 , -0.4455533 , -0.99192095, -1.3812833 ,\n",
       "         0.1473206 , -0.42190146, -1.0411509 , -0.4282124 , -0.08625543,\n",
       "        -0.15660407, -0.671358  ,  0.34606442,  0.3644882 , -0.36716112,\n",
       "        -0.49284393, -0.37988544,  1.0400015 , -0.5381476 , -0.2230277 ,\n",
       "         0.09821817, -0.58493775, -0.20329998, -0.16444077, -1.0804529 ,\n",
       "        -1.055374  ,  0.7685127 ,  0.493413  ,  0.04350467,  1.5034428 ,\n",
       "        -0.5917806 , -0.23430103,  0.606932  , -0.18783182,  0.39633125,\n",
       "        -0.532951  , -1.0264511 ,  1.0419172 ,  0.25539327,  0.36876366,\n",
       "         0.31524554,  0.57924294, -0.16240107,  0.08073205, -0.07738842,\n",
       "         0.953249  ,  0.41010183,  0.10603315,  0.03989341, -1.2710207 ,\n",
       "        -1.3895757 , -0.43576163,  0.2212844 ,  1.0744457 ,  0.44011346,\n",
       "         0.35474893,  0.4384694 ,  0.21386078, -0.7260071 ,  0.27147296,\n",
       "        -0.77483195,  0.66295093,  1.4281679 ,  0.20796578,  0.30975947,\n",
       "         0.5001312 , -0.37279212,  1.0552198 , -1.1741111 , -0.59039927,\n",
       "        -0.3170728 ,  0.05679503, -0.38251665, -0.4922408 ,  0.39949152,\n",
       "         0.18213968, -0.868737  ,  0.6398133 , -0.24637474,  0.39744502,\n",
       "         0.35471517,  0.49497446,  0.518893  ,  0.0680106 ,  0.47220677,\n",
       "        -1.1456809 , -0.06620643, -0.24372476, -0.5177826 , -0.5864774 ,\n",
       "        -0.54983604, -0.4990643 ,  0.81011534, -0.4976548 , -0.5931042 ,\n",
       "        -0.5636542 , -0.19568738, -0.18142328,  0.10816833, -0.7686216 ,\n",
       "        -0.06406418, -0.27345195,  0.49241275,  0.17181163,  0.59614354,\n",
       "         0.70522624,  0.34121114,  0.5419178 , -0.5018094 , -0.59382683,\n",
       "        -0.9388486 ,  0.07489147,  1.33014   ,  0.6423279 ,  0.2790345 ,\n",
       "        -0.34320506, -0.25339767, -1.4093888 , -0.11628159,  0.73781425,\n",
       "         0.07096913,  0.2177098 , -0.7436555 , -0.93111354,  1.2677802 ,\n",
       "        -1.5305358 , -0.7164093 , -0.11248636, -1.0210407 , -0.94985825,\n",
       "        -0.12729949, -0.23509385,  0.48917183,  0.12722278, -0.30641901,\n",
       "        -0.42921314, -1.3053846 ,  0.24779376, -0.33955103,  0.24505788,\n",
       "         0.874684  ,  0.2189305 ,  1.8402975 ,  0.4660055 ,  0.65356475,\n",
       "         0.44178054,  0.9621633 , -0.3881434 ,  0.01141143, -1.093312  ,\n",
       "        -0.14943774,  0.58842045, -0.8857565 , -1.1972762 , -0.96511066,\n",
       "         0.33778656,  0.40822726,  0.08874913,  0.26149213,  1.9271911 ,\n",
       "         0.15520658, -1.0610266 , -0.4502962 , -0.6641976 ,  0.6432283 ,\n",
       "        -0.02144737, -0.5870269 , -0.14166674,  0.5024419 ,  1.1124407 ,\n",
       "        -1.7676784 , -0.3620612 , -0.4007592 , -1.0264312 ,  1.7857537 ,\n",
       "        -0.11041103, -0.13338356,  0.8519382 , -0.47717184, -0.3963852 ,\n",
       "         0.3123894 , -0.4439848 , -0.02385996, -0.31822896,  0.01250023,\n",
       "         0.12413719, -0.40088573,  0.65882945,  0.04193098, -1.6060934 ,\n",
       "         0.5296077 ,  0.05165862,  0.05964223,  0.19352774,  0.3994615 ,\n",
       "        -0.16486482,  1.0122111 , -0.0123807 , -1.1728    , -0.71930623,\n",
       "         0.2061387 , -0.54481244, -0.22510375, -0.9409267 ,  0.61968   ,\n",
       "        -0.72574675,  0.55416477, -0.49848223,  0.7619483 ,  0.30873987,\n",
       "        -0.6432016 , -0.7329245 ,  0.05099602,  0.1705041 , -0.1432941 ,\n",
       "        -0.05891338,  1.235695  , -0.09017479, -0.574994  ,  0.70874506,\n",
       "         0.5231074 ,  0.5842361 , -1.906421  ,  0.52383125,  0.26832873,\n",
       "         0.33494872, -0.215657  ,  0.66737205, -0.9603931 ,  0.9167407 ,\n",
       "         0.00905196,  0.48482126,  0.06829649,  1.2197472 , -1.5459702 ,\n",
       "        -0.9413356 ,  1.1962628 , -0.48179135,  0.74412066, -0.3299172 ,\n",
       "        -0.16528976, -0.17073955, -0.38394418,  0.44025606, -0.9349677 ,\n",
       "         0.3051292 ,  0.20951882,  1.1240373 ,  0.4666654 ,  0.8512685 ,\n",
       "         0.04276197,  0.570626  ,  0.122722  , -0.37072077, -0.41816548,\n",
       "         0.08016404,  0.6959296 , -0.77613133,  0.40642834, -0.36633655,\n",
       "        -0.13821343, -0.37528968,  0.09812314, -0.8345919 ,  0.06391244,\n",
       "        -0.10756801, -0.7420404 ,  0.9367377 ,  0.63208175,  0.9517135 ,\n",
       "        -0.09840345, -0.34349108, -0.5224062 ,  0.4120284 , -0.7212612 ,\n",
       "        -0.20572305,  0.3145908 , -0.33683953,  0.01840206, -0.47848067,\n",
       "        -0.00511419,  0.17198338, -0.19952953,  0.37109816,  0.79998404,\n",
       "        -0.22419636, -0.07041323,  1.4356444 , -0.6569195 ,  0.3872616 ,\n",
       "        -1.729819  , -0.53364223, -0.2075081 , -0.5693584 , -0.8051174 ,\n",
       "         0.98850864, -0.66366893,  1.3228054 , -1.1101061 ,  0.7711826 ,\n",
       "         0.50904804, -0.5083433 , -1.2154901 , -1.2989196 ,  0.33000398,\n",
       "         0.3725409 , -0.58983505, -0.14339995,  0.758043  , -0.39173922,\n",
       "         0.85024583, -0.42726505, -1.049217  , -0.4528435 ,  0.01781281,\n",
       "        -0.744736  , -0.05259502, -0.45265728, -1.4838519 , -0.5652133 ,\n",
       "        -1.5247666 ,  0.29549196, -0.42680848, -1.6406312 , -0.60886455,\n",
       "         0.2977153 , -0.45960045,  0.6817232 , -0.9066102 ,  1.0219954 ,\n",
       "         0.03937908, -0.22762606, -0.80091125, -0.05010664,  0.03400726,\n",
       "        -1.5782038 ,  0.45326105,  1.2512891 ,  0.96264637, -0.30877823,\n",
       "         0.8393267 ,  0.3810619 , -0.87472904,  0.15645857,  0.18890542,\n",
       "        -0.6518444 ,  0.14649494,  0.4963628 , -0.37273747,  0.21719089,\n",
       "        -0.86266494, -0.49379858, -1.4861103 ,  0.3592869 , -0.09554499,\n",
       "        -0.267936  ,  0.47932947, -0.35779208, -0.34573272, -0.91664726,\n",
       "        -0.21585137,  0.11209919, -0.4038211 , -0.34142163,  1.0444756 ,\n",
       "         0.12728347,  0.11390345, -0.7964734 , -0.06490196, -0.10974263,\n",
       "        -0.1930848 , -1.1152077 ], dtype=float32),\n",
       " 'norm': 15.430535,\n",
       " 'bbox': array([ 268,  298,  962, 1223]),\n",
       " 'aligned_face': array([[[205, 190, 183],\n",
       "         [204, 191, 183],\n",
       "         [205, 190, 183],\n",
       "         ...,\n",
       "         [205, 192, 184],\n",
       "         [205, 192, 184],\n",
       "         [207, 192, 185]],\n",
       " \n",
       "        [[204, 191, 183],\n",
       "         [204, 189, 182],\n",
       "         [204, 190, 182],\n",
       "         ...,\n",
       "         [206, 193, 184],\n",
       "         [206, 191, 184],\n",
       "         [207, 192, 185]],\n",
       " \n",
       "        [[204, 191, 183],\n",
       "         [206, 189, 182],\n",
       "         [204, 189, 182],\n",
       "         ...,\n",
       "         [204, 189, 182],\n",
       "         [203, 188, 181],\n",
       "         [205, 190, 183]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 170, 160],\n",
       "         [184, 171, 162],\n",
       "         [184, 171, 163],\n",
       "         ...,\n",
       "         [201, 184, 177],\n",
       "         [196, 182, 176],\n",
       "         [197, 182, 175]],\n",
       " \n",
       "        [[185, 171, 162],\n",
       "         [187, 171, 162],\n",
       "         [185, 172, 164],\n",
       "         ...,\n",
       "         [202, 185, 177],\n",
       "         [198, 184, 176],\n",
       "         [198, 185, 176]],\n",
       " \n",
       "        [[184, 171, 162],\n",
       "         [187, 170, 163],\n",
       "         [184, 172, 160],\n",
       "         ...,\n",
       "         [198, 183, 176],\n",
       "         [199, 184, 177],\n",
       "         [200, 185, 178]]], dtype=uint8),\n",
       " 'gender': 'M',\n",
       " 'age': 39,\n",
       " 'pitch': 0.07648031,\n",
       " 'yaw': 2.7766435,\n",
       " 'roll': -0.7765556,\n",
       " 'fiqa_score': 2.378752}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.process_image_single_face(\"001_frontal.jpg\")\n"
=======
    "ff = ForensicFace(use_gpu=True, extended=True)"
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'bbox', 'aligned_face', 'gender', 'age', 'pitch', 'yaw', 'roll', 'fiqa_score']),\n",
<<<<<<< HEAD
       " array([[471.42743, 418.60498],\n",
       "        [522.68933, 418.05362],\n",
       "        [498.82196, 449.08923],\n",
       "        [479.3499 , 476.4419 ],\n",
       "        [514.33453, 476.06888]], dtype=float32),\n",
=======
       " array([[467.3665 , 681.6384 ],\n",
       "        [772.4218 , 677.03107],\n",
       "        [625.37726, 836.4494 ],\n",
       "        [504.2984 , 992.18964],\n",
       "        [751.0406 , 985.683  ]], dtype=float32),\n",
       " 305.09012,\n",
       " (512,))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ff.process_image_single_face(\"001_frontal.jpg\")\n",
    "result.keys(), result[\"keypoints\"], result[\"ipd\"], result[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'bbox', 'aligned_face', 'gender', 'age', 'pitch', 'yaw', 'roll', 'fiqa_score', 'magface_embedding', 'magface_norm']),\n",
       " array([[471.42743, 418.60498],\n",
       "        [522.68933, 418.05362],\n",
       "        [498.82196, 449.08923],\n",
       "        [479.3499 , 476.44193],\n",
       "        [514.33453, 476.06885]], dtype=float32),\n",
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
       " array([441, 355, 548, 506]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image_multiple_faces(\"tela.png\")\n",
<<<<<<< HEAD
    "results[0].keys(), results[0][\"keypoints\"], results[0][\"bbox\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'bbox', 'aligned_face', 'gender', 'age', 'pitch', 'yaw', 'roll', 'fiqa_score']),\n",
       " array([[103.60016, 139.88235],\n",
       "        [174.26508, 137.3373 ],\n",
       "        [140.28102, 187.14735],\n",
       "        [109.09419, 219.34006],\n",
       "        [173.40771, 217.09572]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image_single_face(\"obama.png\")\n",
    "results.keys(), results[\"keypoints\"]\n"
=======
    "results[0].keys(), results[0][\"keypoints\"], results[0][\"bbox\"]"
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre duas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def compare(self: ForensicFace, img1path: str, img2path: str):\n",
    "    \"\"\"\n",
    "    Compares the similarity between two face images based on their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        - img1path (str): Path to the first image file\n",
    "        - img2path (str): Path to the second image file\n",
    "\n",
    "    Returns:\n",
    "        A float representing the similarity score between the two faces based on their embeddings.\n",
    "        The score ranges from -1.0 to 1.0, where 1.0 represents a perfect match and -1.0 represents a complete mismatch.\n",
    "    \"\"\"\n",
    "    img1data = self.process_image(img1path)\n",
    "    assert len(img1data) > 0, f\"No face detected in {img1path}\"\n",
    "    img2data = self.process_image(img2path)\n",
    "    assert len(img2data) > 0, f\"No face detected in {img2path}\"\n",
    "    return np.dot(img1data[\"embedding\"], img2data[\"embedding\"]) / (\n",
    "        img1data[\"norm\"] * img2data[\"norm\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0.8556379"
=======
       "0.85562766"
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.compare(\"obama.png\", \"obama2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregação de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_embeddings(self: ForensicFace, embeddings, weights=None, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates multiple embeddings into a single embedding.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): A 2D array of shape (num_embeddings, embedding_dim) containing the embeddings to be\n",
    "            aggregated.\n",
    "        weights (numpy.ndarray, optional): A 1D array of shape (num_embeddings,) containing the weights to be assigned\n",
    "            to each embedding. If not provided, all embeddings are equally weighted.\n",
    "\n",
    "        method (str, optional): choice of agregating based on the mean or median of the embeddings. Possible values are\n",
    "            'mean' and 'median'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 1D array of shape (embedding_dim,) containing the aggregated embedding.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(embeddings.shape[0], dtype=\"int\")\n",
    "    assert embeddings.shape[0] == weights.shape[0]\n",
    "    assert method in [\"mean\", \"median\"]\n",
    "    if method == \"mean\":\n",
    "        return np.average(embeddings, axis=0, weights=weights)\n",
    "    else:\n",
    "        weighted_embeddings = np.array([w * e for w, e in zip(weights, embeddings)])\n",
    "        return np.median(weighted_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_from_images(\n",
    "    self: ForensicFace, list_of_image_paths, method=\"mean\", quality_weight=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a list of image paths, this method returns the average embedding of all faces found in the images.\n",
    "\n",
    "    Args:\n",
    "        list_of_image_paths (List[str]): List of paths to images.\n",
    "        method (str, optional): choice of agregating based on the mean or median of the embeddings. Possible values are\n",
    "            'mean' and 'median'.\n",
    "        quality_weight (boolean, optional): If True, use the FIQA(L) score as a weight for aggregation.\n",
    "\n",
    "    Returns:\n",
    "        Union[np.ndarray, List]: If one or more faces are found, returns a 1D numpy array of shape (512,) representing the\n",
    "        average embedding. Otherwise, returns an empty list.\n",
    "    \"\"\"\n",
    "    if quality_weight:\n",
    "        assert (\n",
    "            self.extended == True\n",
    "        ), \"You must initialize ForensicFace with extended = True\"\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    for imgpath in list_of_image_paths:\n",
    "        d = self.process_image(imgpath)\n",
    "        if len(d) > 0:\n",
    "            embeddings.append(d[\"embedding\"])\n",
    "            weights.append(d[\"fiqa_score\"] if quality_weight == True else 1.0)\n",
    "    if len(embeddings) > 0:\n",
    "        return self.aggregate_embeddings(\n",
    "            np.array(embeddings), method=method, weights=np.array(weights)\n",
    "        )\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
=======
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/miniconda3/envs/ffdev/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "aggregated = ff.aggregate_from_images([\"obama.png\", \"obama2.png\"])\n",
    "aggregated.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suporte a MagFace\n",
    "\n",
    "Para utilizar, instancie o forensicface com a opção magface = True:\n",
    "\n",
    "``ff = forensicface(magface=True)``\n",
    "\n",
    "Modelo de [MagFace](https://github.com/IrvingMeng/MagFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.226337, 22.591608)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = ForensicFace(det_size=320, use_gpu=True, magface=True)\n",
    "good = ff.process_image(\"001_frontal.JPG\")\n",
    "bad = ff.process_image(\"001_cam1_1.jpg\")\n",
    "good[\"magface_norm\"], bad[\"magface_norm\"]\n"
=======
    "aggregated = ff.aggregate_from_images([\"obama.png\", \"obama2.png\"], quality_weight=True)\n",
    "aggregated.shape"
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de faces de vídeos com margem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_extended_bbox(self: ForensicFace, bbox, frame_shape, margin_factor):\n",
    "    \"\"\"\n",
    "    Computes and returns the bounding box with extended margins.\n",
    "\n",
    "    Parameters:\n",
    "        bbox (ndarray): The bounding box coordinates (startX, startY, endX, endY).\n",
    "        frame_shape (tuple): The shape of the video frame (height, width, channels).\n",
    "        margin_factor (float): The factor to be applied for computing the margin.\n",
    "\n",
    "    Returns:\n",
    "        A list with the coordinates of the extended bounding box (startX_out, startY_out, endX_out, endY_out).\n",
    "    \"\"\"\n",
    "    # add a margin on the bounding box\n",
    "    (startX, startY, endX, endY) = bbox.astype(\"int\")\n",
    "    (h, w) = frame_shape[:2]\n",
    "    out_width = (endX - startX) * margin_factor\n",
    "    out_height = (endY - startY) * margin_factor\n",
    "\n",
    "    startX_out = int((startX + endX) / 2 - out_width / 2)\n",
    "    endX_out = int((startX + endX) / 2 + out_width / 2)\n",
    "    startY_out = int((startY + endY) / 2 - out_height / 2)\n",
    "    endY_out = int((startY + endY) / 2 + out_height / 2)\n",
    "\n",
    "    # tests if the output bbox coordinates are out of frame limits\n",
    "    if startX_out < 0:\n",
    "        startX_out = 0\n",
    "    if endX_out > int(w):\n",
    "        endX_out = int(w)\n",
    "    if startY_out < 0:\n",
    "        startY_out = 0\n",
    "    if endY_out > int(h):\n",
    "        endY_out = int(h)\n",
    "    return [startX_out, startY_out, endX_out, endY_out]\n",
    "\n",
    "\n",
    "@patch\n",
    "def extract_faces(\n",
    "    self: ForensicFace,\n",
    "    video_path: str,  # path to video file\n",
    "    dest_folder: str = None,  # folder used to save extracted faces. If not provided, a new folder with the video name is created\n",
    "    every_n_frames: int = 1,  # skip some frames\n",
    "    margin: float = 2.0,  # margin to add to each face, w.r.t. detected bounding box\n",
    "    start_from: float = 0.0,  # seconds after video start to begin processing\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts faces from a video and saves them as individual images.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): The path to the input video file.\n",
    "        dest_folder (str, optional): The path to the output folder. If not provided, a new folder with the same name as the input video file is created.\n",
    "        every_n_frames (int, optional): Extract faces from every n-th frame. Default is 1 (extract faces from all frames).\n",
    "        margin (float, optional): The factor by which the detected face bounding box should be extended. Default is 2.0.\n",
    "        start_from (float, optional): The time point (in seconds) after which the video frames should be processed. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        The number of extracted faces.\n",
    "    \"\"\"\n",
    "    if dest_folder is None:\n",
    "        dest_folder = os.path.splitext(video_path)[0]\n",
    "\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "    # initialize video stream from file\n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(fps * start_from)\n",
    "\n",
    "    # seek to starting frame\n",
    "    vs.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    current_frame = start_frame\n",
    "    nfaces = 0\n",
    "    while True:\n",
    "        if (current_frame % every_n_frames) != 0:\n",
    "            current_frame = current_frame + 1\n",
    "            continue\n",
    "\n",
    "        vs.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        ret, frame = vs.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        current_frame = current_frame + 1\n",
    "        (h, w) = frame.shape[:2]\n",
    "\n",
    "        faces = self.detectmodel.get(frame)\n",
    "        for i, face in enumerate(faces):\n",
    "            startX, startY, endX, endY = face.bbox.astype(\"int\")\n",
    "            faceW = endX - startX\n",
    "            faceH = endY - startY\n",
    "            outBbox = self._get_extended_bbox(\n",
    "                face.bbox, frame.shape, margin_factor=margin\n",
    "            )\n",
    "            # export the face (with added margin)\n",
    "            face_crop = frame[outBbox[1] : outBbox[3], outBbox[0] : outBbox[2]]\n",
    "            face_img_path = os.path.join(\n",
    "                dest_folder, f\"frame_{current_frame:07}_face_{i:02}.png\"\n",
    "            )\n",
    "            cv2.imwrite(face_img_path, face_crop)\n",
    "            nfaces += 1\n",
    "    vs.release()\n",
    "    return nfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\ramys/.insightface\\models\\sepaelv2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.extract_faces(\n",
    "    video_path=\"../video.mp4\",\n",
    "    start_from=0,\n",
    "    every_n_frames=1,\n",
<<<<<<< HEAD
    "    dest_folder=\"../video_faces_output\",\n",
    ")\n"
=======
    "    dest_folder=\"/home/rafael/video_faces\",\n",
    ")"
>>>>>>> 0a6791b60984b4700b8c7474ab77b0d0900da252
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
